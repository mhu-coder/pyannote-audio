#!/usr/bin/env python
# encoding: utf-8

# The MIT License (MIT)

# Copyright (c) 2016-2017 CNRS

# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:

# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.

# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# AUTHORS
# HervÃ© BREDIN - http://herve.niderb.fr

import torch
import torch.nn as nn
from torch.autograd import Variable

import numpy as np

from pyannote.core import SlidingWindow, SlidingWindowFeature

from pyannote.generators.batch import FileBasedBatchGenerator
from pyannote.generators.fragment import SlidingSegments

from pyannote.audio.generators.periodic import PeriodicFeaturesMixin
from pyannote.audio.callback import LoggingCallback

from pyannote.database import get_unique_identifier

# import torch
# import torch.nn as nn
# from torch.autograd import Variable


class SequenceLabeling(PeriodicFeaturesMixin, FileBasedBatchGenerator):
    """Sequence labeling

    Parameters
    ----------
    model : keras.Model
        Pre-trained sequence labeling model.
    feature_extraction : callable
        Feature extractor
    duration : float
        Subsequence duration, in seconds.
    step : float, optional
        Subsequence step, in seconds. Defaults to 50% of `duration`.
    batch_size : int, optional
        Defaults to 32.
    gpu : boolean, optional
        Run on GPU. Only works witht pytorch backend.

    Usage
    -----
    >>> model = keras.models.load_model(...)
    >>> feature_extraction = YaafeMFCC(...)
    >>> sequence_labeling = SequenceLabeling(model, feature_extraction, duration)
    >>> sequence_labeling.apply(current_file)
    """

    def __init__(self, model, feature_extraction, duration,
                 step=None, batch_size=32, source='audio', gpu=False):

        self.model = model.cuda() if gpu else model
        self.feature_extractor = feature_extraction
        self.duration = duration
        self.batch_size = batch_size
        self.gpu = gpu

        generator = SlidingSegments(duration=duration, step=step, source=source)
        self.step = generator.step if step is None else step

        super(SequenceLabeling, self).__init__(
            generator, batch_size=self.batch_size)

    @property
    def dimension(self):
        if isinstance(self.model, nn.Module):
            return self.model.n_classes
        else:
            return self.model.output_shape[-1]

    @property
    def sliding_window(self):
        return self.feature_extractor.sliding_window()

    def signature(self):
        shape = self.shape
        return {'type': 'ndarray', 'shape': self.shape}

    def postprocess_ndarray(self, X):
        """Label sequences

        Parameter
        ---------
        X : (batch_size, n_samples, n_features) numpy array
            Batch of input sequences

        Returns
        -------
        prediction : (batch_size, n_samples, dimension) numpy array
            Batch of sequence labelings.
        """
        if isinstance(self.model, nn.Module):
            X = Variable(torch.from_numpy(np.rollaxis(np.array(X, dtype=np.float32), 0, 2)))
            if self.gpu:
                prediction = self.model(X.cuda()).data.cpu().numpy()
            else:
                prediction = self.model(X).data.numpy()
            return np.rollaxis(prediction, 1, 0)
        else:
            return self.model.predict(X)

    def apply(self, current_file):
        """Compute predictions on a sliding window

        Parameter
        ---------
        current_file : dict

        Returns
        -------
        predictions : SlidingWindowFeature
        """

        # frame and sub-sequence sliding windows
        frames = self.feature_extractor.sliding_window()

        batches = [batch for batch in self.from_file(current_file,
                                                     incomplete=True)]
        if not batches:
            data = np.zeros((0, self.dimension), dtype=np.float32)
            return SlidingWindowFeature(data, frames)

        fX = np.vstack(batches)

        subsequences = SlidingWindow(duration=self.duration, step=self.step)

        # get total number of frames
        identifier = get_unique_identifier(current_file)
        n_frames = self.preprocessed_['X'][identifier].data.shape[0]

        # data[i] is the sum of all predictions for frame #i
        data = np.zeros((n_frames, self.dimension), dtype=np.float32)

        # k[i] is the number of sequences that overlap with frame #i
        k = np.zeros((n_frames, 1), dtype=np.int8)

        for subsequence, fX_ in zip(subsequences, fX):

            # indices of frames overlapped by subsequence
            indices = frames.crop(subsequence,
                                  mode='center',
                                  fixed=self.duration)

            # accumulate the outputs
            data[indices] += fX_

            # keep track of the number of overlapping sequence
            # TODO - use smarter weights (e.g. Hamming window)
            k[indices] += 1

        # compute average embedding of each frame
        data = data / np.maximum(k, 1)

        return SlidingWindowFeature(data, frames)

    @classmethod
    def train(cls, input_shape, design_model, generator, steps_per_epoch,
              epochs, loss='categorical_crossentropy', optimizer='rmsprop',
              log_dir=None):
        """Train the model

        Parameters
        ----------
        input_shape : (n_frames, n_features) tuple
            Shape of input sequence
        design_model : function or callable
            This function should take input_shape as input and return a Keras
            model that takes a sequence as input, and returns the labeling as
            output.
        generator : iterable
            The output of the generator must be a tuple (inputs, targets) or a
            tuple (inputs, targets, sample_weights). All arrays should contain
            the same number of samples. The generator is expected to loop over
            its data indefinitely. An epoch finishes when `steps_per_epoch`
            samples have been seen by the model.
        steps_per_epoch : int
            Number of batches to process before going to the next epoch.
        epochs : int
            Total number of iterations on the data
        optimizer: str, optional
            Keras optimizer. Defaults to 'rmsprop'.
        log_dir: str, optional
            When provided, log status after each epoch into this directory.
            This will create several files, including loss plots and weights
            files.

        See also
        --------
        keras.engine.training.Model.fit_generator
        """

        callbacks = []

        if log_dir is not None:
            log = [('train', 'loss'), ('train', 'accuracy')]
            callback = LoggingCallback(log_dir, log=log)
            callbacks.append(callback)

        # in case the {generator | optimizer} define their own
        # callbacks, append them as well. this might be useful.
        for stuff in [generator, optimizer]:
            if hasattr(stuff, 'callbacks'):
                callbacks.extend(stuff.callbacks())

        model = design_model(input_shape)
        model.compile(optimizer=optimizer,
                               loss=loss,
                               metrics=['accuracy'])

        return model.fit_generator(
            generator, steps_per_epoch, epochs=epochs,
            verbose=1, callbacks=callbacks)
